{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "particle_jet_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bencammett/ML_project_Comp2/blob/master/particle_jet_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uVfSWGCJf49J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "based on code (imdb movie reviews) from: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_text_classification.ipynb\n",
        "\n",
        "running in google collab"
      ]
    },
    {
      "metadata": {
        "id": "0OtegxR9f03W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1b116010-9a54-44c7-a0f3-48fcbf4753be"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MK_SOEb0gNE6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load dataset \n",
        "\n",
        "(**Need to be preprocessed? **)\n",
        "\n",
        "Integers will correlate to name of a particle\n",
        "\n",
        "Pythia to generate events (**how many?**) for training -- **data for testing as well?**\n",
        "\n",
        "Example from imdb -- (shouldn't need to place maximum, we want all particles, not just \"most frequent\" particles):"
      ]
    },
    {
      "metadata": {
        "id": "PmoAztAJf38x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# imdb = keras.datasets.imdb\n",
        "\n",
        "# (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aqFC4TPDhpnk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(**Optimal number of epochs to avoid overfitting?**)\n",
        "\n",
        "(**Padding -- jets will not have same number of particles**)\n",
        "\n",
        "(**ReLU then Sigmoid? Softmax for probability?**)\n",
        "\n",
        "NN should be able to figure out which jet based on particles returned -- couple catagories, ex: e+e- is fine for test, but extend past that as well\n",
        "\n",
        "Input will just be vector of particle-id "
      ]
    },
    {
      "metadata": {
        "id": "lc9AmvqCJTft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "use to upload data:"
      ]
    },
    {
      "metadata": {
        "id": "QHV5zUmGJWEl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(\"Z_ee_id_only.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8lEmhW8BvoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "9df6fc6e-4d6b-4bc1-8ac0-33824ed34eee"
      },
      "cell_type": "code",
      "source": [
        "text_file = open(\"Z_ee_id_only.txt\", \"r\")\n",
        "lines = text_file.read().split('\\n')\n",
        "lines.remove('')\n",
        "part_id = list(map(int, lines))\n",
        "print(part_id)\n",
        "print(len(part_id))\n",
        "text_file.close()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[23, 2, -2, 23, 21, 2, -2, 23, 21, 21, 21, 21, 21, 2, -2, 23, 21, 21, 21, 21, 21, 21, 21, 21, 21, 2, -1, 1, 21, 21, 21, 1, 21, 21, 21, 21, 21, 21, -1, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 2, -2, 23, 21, 21, -1, 21, 21, 21, 1, 21, 21, 21, 21, 21, 21, 21, 2103, 2, 2101, 2, 11, -11, -11, 22, 11, 11, 22, -11, -11, 22, 11, -1, 2, 21, 21, -1, 223, 331, 321, -313, 223, 2214, -2214, -213, 113, 211, 2, 2101, 211, -211, 223, 213, -213, 223, 211, 331, 113, 2112, 1, 21, 21, 21, 21, 21, 21, 21, 21, 21, 2103, 111, 311, -321, 111, 213, 111, 111, 223, 111, 311, -321, 3212, -3312, -321, 211, 311, 3122, -2212, 211, 311, -313, -211, 321, -321, 211, 1114, 211, -321, 211, 2112, 211, -2212, 111, -211, 111, 211, -211, 211, 111, -211, 111, 211, -211, 130, 211, 111, 310, 130, 130, -321, 211, 2112, -211, 211, -211, 111, 113, 22, 211, -211, 111, 211, -211, 111, 211, -211, 111, 113, 22, 22, 22, 22, 22, 22, 22, 22, 22, 211, -211, 111, 22, 22, 3122, 22, -3122, 211, 2112, 111, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 211, -211, 22, 22, 211, -211, 22, 22, 22, 22, 22, 11, -11, 211, -211, 22, 22, 2212, -211, -2212, 211, 22, 22, 0, 0, 0, 0]\n",
            "245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aQWlJbWVgwjm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from: https://keras.io/layers/embeddings/\n",
        "# modify to fit particle vectors\n",
        "\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(1000, 64, input_length=10))\n",
        "# the model will take as input an integer matrix of size (batch, input_length).\n",
        "# the largest integer (i.e. word index) in the input should be\n",
        "# no larger than 999 (vocabulary size).\n",
        "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
        "\n",
        "\n",
        "# this array should be the particle-id\n",
        "input_array = np.random.randint(1000, size=(32, 10))\n",
        "\n",
        "model.compile('rmsprop', 'mse')\n",
        "output_array = model.predict(input_array)\n",
        "assert output_array.shape == (32, 10, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BwuzOI1T-hKU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}